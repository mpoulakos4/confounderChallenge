---
title: "Using Randomization & Stratification to Overcome A Common Cause Confounder"
format:
  html: default
bibliography: references.bib
execute:
  echo: false
  eval: true
---

## The Paradox: Who *Really* Is the Better Surgeon?

::: {.callout-tip title="A Tale of Two Surgeons" icon=false}
*Paraphrased from* [@taleb2017surgeons]

<div style="float: right; margin-left: 15px; margin-bottom: 10px;">
<img src="docSideBySide.jpg" alt="Doc Dreamy and Doc Duck side by side" style="max-width: 250px; height: auto; border-radius: 5px;">
</div>

Imagine you need to choose between two surgeons of similar rank at the same hospital. The first surgeon, Doc Dreamy, matches our stereotype perfectly: refined appearance, silver-rimmed glasses, delicate hands, measured speech, and an office adorned with Ivy League diplomas (see the image). The second surgeon, Doc Duck, by contrast, looks more like a butcher—overweight, with large hands, an unkempt appearance, and no visible credentials on the wall.

Counterintuitively, the surgeon who doesn't "look the part" may actually be the better choice. Why? Because when someone succeeds in their profession despite not fitting the expected appearance, it suggests they had to overcome significant perceptual biases. And if we are lucky enough to have people who do not look the part, it is thanks to the presence of some skin in the game, the contact with reality that filters out incompetence. [@taleb2017surgeons]
:::

## Observational Data: A Misleading Victory for Doc Dreamy

```{python}
#| label: simulate-data
#| echo: false
#| message: false
#| warning: false
#| include: false
#| eval: false

import pandas as pd
import numpy as np

# Set seed for reproducibility
np.random.seed(123)

# Create dataframe with 100 patients and severity scores
n_patients = 100
patients_df = (
    pd.DataFrame({
        'patient': range(1, n_patients + 1),
        'severity': np.random.normal(0, 1, n_patients)
    })
    .assign(
        # Assign doctors based on severity using sigmoid function:
        # Probability of Doc Duck is inversely proportional to severity
        # Higher severity → higher probability of Doc Duck
        # Uses sigmoid: P(Doc Duck) = 1 / (1 + exp(-k * severity))
        # where k controls the steepness of the relationship
        prob_duck=lambda df: 1 / (1 + np.exp(-1.5 * df['severity'])),  # k=1.5 controls sensitivity
        # Random assignment: if random < prob_duck → Doc Duck (id=0), else Doc Dreamy (id=1)
        doctor_id=lambda df: (np.random.random(n_patients) >= df['prob_duck']).astype(int),
        # Assign doctor names based on id
        doctor_name=lambda df: np.where(df['doctor_id'] == 1, 'Doc Dreamy', 'Doc Duck'),
        # Assign surgical goodness:
        # Doc Duck: Normal(0.2, 1), Doc Dreamy: Normal(-0.2, 1)
        surgicalGoodness=lambda df: np.where(
            df['doctor_name'] == 'Doc Duck',
            np.random.normal(0.5, 1, n_patients),
            np.random.normal(-0.4, 1, n_patients)
        ),
        # Calculate post-surgical severity: initial severity - surgical goodness + 3
        post_surgical_score=lambda df: df['severity'] - df['surgicalGoodness'] + 3
    )
)

# Save to CSV, excluding prob_duck and surgicalGoodness columns
patients_df[['patient', 'severity', 'doctor_id', 'doctor_name', 'post_surgical_score']].to_csv(
    'patients_data.csv', index=False
)

# Display first few rows
patients_df.head()
```

```{r}
#| label: load-data-r
#| echo: false

patients_df <- read.csv('patients_data.csv')
```


**Plot showing Post-Surgical Outcomes by Patient:** 

```{r}
#| label: fig-plot-outcomes
#| fig-cap: "Post-Surgical Symptom Score (lower is better) - Observed Data"
#| echo: false
#| fig-width: 10
#| fig-height: 6

library(ggplot2)

# Separate data by doctor
dreamy_data <- patients_df[patients_df$doctor_name == 'Doc Dreamy', ]
duck_data <- patients_df[patients_df$doctor_name == 'Doc Duck', ]

# Calculate means
dreamy_mean <- mean(dreamy_data$post_surgical_score)
duck_mean <- mean(duck_data$post_surgical_score)

# Create plot
p1 <- ggplot(patients_df, aes(x = patient, y = post_surgical_score, 
                               color = doctor_name, shape = doctor_name)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_hline(yintercept = dreamy_mean, color = '#4E79A7', linetype = 'dashed', linewidth = 1) +
  geom_hline(yintercept = duck_mean, color = '#E15759', linetype = 'dashed', linewidth = 1) +
  annotate('text', x = max(patients_df$patient) * 0.95, y = dreamy_mean, 
           label = paste('○', round(dreamy_mean, 2)), 
           color = '#4E79A7', fontface = 'bold', size = 3.5,
           hjust = 1, vjust = 0.5) +
  annotate('text', x = max(patients_df$patient) * 0.95, y = duck_mean, 
           label = paste('△', round(duck_mean, 2)), 
           color = '#E15759', fontface = 'bold', size = 3.5,
           hjust = 1, vjust = 0.5) +
  scale_color_manual(values = c('Doc Dreamy' = '#4E79A7', 'Doc Duck' = '#E15759'),
                     name = 'Doctor') +
  scale_shape_manual(values = c('Doc Dreamy' = 19, 'Doc Duck' = 17),
                     name = 'Doctor') +
  labs(x = 'Patient Number', 
       y = 'Post-Surgical Symptom Score (Lower is Better)',
       title = 'Post-Surgical Symptom Score by Patient and Doctor') +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = 'bold', hjust = 0.5, margin = margin(b = 15)),
    axis.title = element_text(size = 12, face = 'bold'),
    axis.text = element_text(size = 10),
    legend.position = 'right',
    legend.title = element_text(face = 'bold'),
    panel.grid.minor = element_blank(),
    panel.grid.major = element_line(color = 'gray90', linewidth = 0.5)
  )

print(p1)
```

## The Hidden Confounder: Patient Severity Explains It All

The easiest path for our brains is to accept the mental model of surgical outcomes depicted in @fig-example-node.

![DAG Model Explaining Surgical Outcomes](dag-example-node.png){#fig-example-node width=50%}

The obvious conclusion is that Doc Dreamy is the superior surgeon because his patients' scores are lower than Doc Duck's patients' scores.

However, a "common cause" confounder can create an association between $X$ and $Y$ that is not causal in nature. For example, if $X$ is puddles on the road and $Y$ is people with umbrellas, it does not mean that the puddles cause people to have umbrellas. Instead, a common cause for both, namely rain ($Z$), is the sole reason for the observed association. The presence of rain causes both puddles and people to carry umbrellas, creating a spurious correlation between the two.

As Nassim Taleb hints, there may be an alternate explanation. Consider the model shown in @fig-example2-node.  Might there be an unaccounted for common cause that is causing the association between surgeon choice and surgical outcomes?

![DAG Model Explaining Surgical Outcomes](dag-example2-node.png){#fig-example2-node width=50%}

A possible common-cause story would go something like this. Both surgeons have full schedules, with Dr. Dreamy scheduling surgeries 3 weeks in advance and Dr. Duck scheduling surgeries 1 week in advance. As such, patients who are not in a rush, usually those with low severity, are more likely to choose Dr. Dreamy based on his website and picture. However, patients who are in more of a rush, usually those with high severity, are more likely to choose Dr. Duck based on his availability and the fact that he is the only surgeon who can see them immediately.

## First Solution: Randomization — Break the Confounding Path

The gold standard to establish causation is a randomized controlled trial. Instead of letting patients choose their surgeon, we randomly assign them to either Dr. Dreamy or Dr. Duck.

This is precisely the kind of problem randomization can solve. By randomly assigning patients to surgeons, we break the potential confounding relationship where patient severity ends up correlated with choice of surgeon. Since assignment is now random rather than based on patient choice between a good-looking doctor and a doctor available more quickly, we eliminate this confounding pathway. This break, pictured in @fig-example3-node, allows us to assess whether surgeon identity truly matters for surgical success, or if the observed association was merely due to patient severity influencing both surgeon choice and outcomes.

![DAG Model Explaining Surgical Outcomes](dag-example3-node.png){#fig-example3-node width=65%}


@fig-example3-node shows the DAG model for the randomization scenario. In this scenario, patient severity ($Z$) is no longer a cause of surgeon choice ($X$). Instead, randomization ($R$) completely determines surgeon assignment ($X$). This breaks the confounding relationship between patient severity and surgeon choice, allowing us to assess whether the surgeons themselves truly matter for surgical outcomes.

### Our data doesn't have counterfactuals, how can we know what would have happened to those patients if we had randomized?

Unfortunately, we can't observe what didn't happen. The patients we already observed went to the surgeon they chose (or who was available), and we'll never know their outcomes under the alternative assignment. This is the fundamental problem of causal inference: we only see one world, not the parallel universe where everything else was held constant but the treatment differed.

```{python}
#| label: simulate-data-randomized
#| echo: true
#| message: false
#| warning: false
#| include: false
#| eval: false

import pandas as pd
import numpy as np

# Set seed for reproducibility (different from observational data)
np.random.seed(456)

# Create dataframe with 100 patients and severity scores
n_patients_randomized = 100
patients_randomized_df = (
    pd.DataFrame({
        'patient': range(1, n_patients_randomized + 1),
        'severity': np.random.normal(0, 1, n_patients_randomized)
    })
    .assign(
        # Randomly assign doctors: 50/50 chance, independent of severity
        # This breaks the confounding relationship
        doctor_id=lambda df: np.random.choice([0, 1], size=n_patients_randomized),
        # Assign doctor names based on id
        doctor_name=lambda df: np.where(df['doctor_id'] == 1, 'Doc Dreamy', 'Doc Duck'),
        # Assign surgical goodness:
        # Doc Duck: Normal(0.5, 1), Doc Dreamy: Normal(-0.4, 1)
        # Same true effect as before - Doc Duck is better
        surgicalGoodness=lambda df: np.where(
            df['doctor_name'] == 'Doc Duck',
            np.random.normal(0.5, 1, n_patients_randomized),
            np.random.normal(-0.4, 1, n_patients_randomized)
        ),
        # Calculate post-surgical severity: initial severity - surgical goodness + 3
        post_surgical_score=lambda df: df['severity'] - df['surgicalGoodness'] + 3
    )
)

# Save to CSV, excluding surgicalGoodness column
patients_randomized_df[['patient', 'severity', 'doctor_id', 'doctor_name', 'post_surgical_score']].to_csv(
    'patients_data_randomized.csv', index=False
)

```

```{r}
#| label: load-data-randomized-r
#| echo: false

# Load the randomized data
patients_randomized_df <- read.csv('patients_data_randomized.csv')


```


### Randomized Assignment Outcomes


```{r}
#| label: fig-plot-outcomes-randomized
#| fig-cap: "Post-Surgical Symptom Score (lower is better) for randomized patients."
#| echo: false
#| fig-width: 10
#| fig-height: 6

library(ggplot2)

# Separate data by doctor
dreamy_data_rand <- patients_randomized_df[patients_randomized_df$doctor_name == 'Doc Dreamy', ]
duck_data_rand <- patients_randomized_df[patients_randomized_df$doctor_name == 'Doc Duck', ]

# Calculate means
dreamy_mean_rand <- mean(dreamy_data_rand$post_surgical_score)
duck_mean_rand <- mean(duck_data_rand$post_surgical_score)

# Create plot
p2 <- ggplot(patients_randomized_df, aes(x = patient, y = post_surgical_score, 
                                         color = doctor_name, shape = doctor_name)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_hline(yintercept = dreamy_mean_rand, color = '#4E79A7', linetype = 'dashed', linewidth = 1) +
  geom_hline(yintercept = duck_mean_rand, color = '#E15759', linetype = 'dashed', linewidth = 1) +
  annotate('text', x = max(patients_randomized_df$patient) * 0.95, y = dreamy_mean_rand, 
           label = paste('○', round(dreamy_mean_rand, 2)), 
           color = '#4E79A7', fontface = 'bold', size = 3.5,
           hjust = 1, vjust = 0.5) +
  annotate('text', x = max(patients_randomized_df$patient) * 0.95, y = duck_mean_rand, 
           label = paste('△', round(duck_mean_rand, 2)), 
           color = '#E15759', fontface = 'bold', size = 3.5,
           hjust = 1, vjust = 0.5) +
  scale_color_manual(values = c('Doc Dreamy' = '#4E79A7', 'Doc Duck' = '#E15759'),
                     name = 'Doctor') +
  scale_shape_manual(values = c('Doc Dreamy' = 19, 'Doc Duck' = 17),
                     name = 'Doctor') +
  labs(x = 'Patient Number', 
       y = 'Post-Surgical Symptom Score (Lower is Better)',
       title = 'Post-Surgical Symptom Score (Randomized Assignment)') +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = 'bold', hjust = 0.5, margin = margin(b = 15)),
    axis.title = element_text(size = 12, face = 'bold'),
    axis.text = element_text(size = 10),
    legend.position = 'right',
    legend.title = element_text(face = 'bold'),
    panel.grid.minor = element_blank(),
    panel.grid.major = element_line(color = 'gray90', linewidth = 0.5)
  )

print(p2)
```

Doc Duck's average post-surgical symptom score is 2.71 while Doc Dreamy's average is 3.46. Doc Duck performs better since lower scores indicate better outcomes. With randomization breaking the confounding relationship, we can now properly assess the causal effect. A two-sample t-test reveals a statistically significant difference (t = 2.734, p = 0.007).  Not only is Dr. Dreamy not the better surgeon, he is actually the worse surgeon.

##  Second Solution: Stratification — When You Can't Randomize

The key to handling a common cause confounder is to stratify by the common cause. In general, this means we examine the relationship between treatment and outcome within groups that share the same value of the confounder. In our example, this means we look at patients of similar severity and compare the outcomes of the surgeons for patients of similar severity. There are mathematically sophisticated ways to do this, but here we demonstrate it visually. Since patient number lacks inherent meaning, the following figure shows patient severity on the x-axis and post-surgical symptom score on the y-axis.  

```{r}
#| label: fig-plot-outcomes-severity
#| fig-cap: "Post-Surgical Symptom Score (lower is better) by Patient and Severity"
#| echo: false
#| fig-width: 10
#| fig-height: 6

library(ggplot2)

# Separate data by doctor (using observational data)
dreamy_data_sev <- patients_df[patients_df$doctor_name == 'Doc Dreamy', ]
duck_data_sev <- patients_df[patients_df$doctor_name == 'Doc Duck', ]

# Calculate means
dreamy_mean_sev <- mean(dreamy_data_sev$post_surgical_score)
duck_mean_sev <- mean(duck_data_sev$post_surgical_score)

# Create plot
p3 <- ggplot(patients_df, aes(x = severity, y = post_surgical_score, 
                               color = doctor_name, shape = doctor_name)) +
  annotate('rect', xmin = -1, xmax = 1, ymin = -Inf, ymax = Inf, 
           alpha = 0.15, fill = 'gray', color = NA) +
  geom_point(size = 3, alpha = 0.7) +
  geom_hline(yintercept = dreamy_mean_sev, color = '#4E79A7', linetype = 'dashed', linewidth = 1) +
  geom_hline(yintercept = duck_mean_sev, color = '#E15759', linetype = 'dashed', linewidth = 1) +
  annotate('text', x = max(abs(patients_df$severity)) * 0.95, y = dreamy_mean_sev, 
           label = paste('○', round(dreamy_mean_sev, 2)), 
           color = '#4E79A7', fontface = 'bold', size = 3.5,
           hjust = 1, vjust = 0.5) +
  annotate('text', x = max(abs(patients_df$severity)) * 0.95, y = duck_mean_sev, 
           label = paste('△', round(duck_mean_sev, 2)), 
           color = '#E15759', fontface = 'bold', size = 3.5,
           hjust = 1, vjust = 0.5) +
  scale_color_manual(values = c('Doc Dreamy' = '#4E79A7', 'Doc Duck' = '#E15759'),
                     name = 'Doctor') +
  scale_shape_manual(values = c('Doc Dreamy' = 19, 'Doc Duck' = 17),
                     name = 'Doctor') +
  labs(x = 'Initial Patient Severity', 
       y = 'Post-Surgical Symptom Score (Lower is Better)',
       title = 'Post-Surgical Symptom Score by Patient Severity') +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = 'bold', hjust = 0.5, margin = margin(b = 15)),
    axis.title = element_text(size = 12, face = 'bold'),
    axis.text = element_text(size = 10),
    legend.position = 'right',
    legend.title = element_text(face = 'bold'),
    panel.grid.minor = element_blank(),
    panel.grid.major = element_line(color = 'gray90', linewidth = 0.5)
  )

print(p3)
```

Within the highlighted region, Doc Dreamy's blue circles tend to be higher (worse) than Doc Duck's red triangles for these patients. It is also obvious that Doc Duck is seeing the more severe patients overall, as his red triangles are generally to the right of Doc Dreamy's blue circles on the x-axis. This is exactly what we would expect if patient severity were a common cause of surgeon choice and surgical outcomes. Despite the aggregate statistics pointing to Doc Dreamy having lower (better) post-surgical symptom scores on average, our visual analysis of this overlapping region leads us to conclude that Doc Duck is actually the better surgeon—a conclusion that would be obscured if we only looked at the means without stratifying by severity.

## Conclusion

The problem with graphs and statistics in general is that the numbers can easily mislead you. Or rather, you can be misled by the numbers when you don't understand what they're really telling you.

When you just look at the raw data and ask who had better outcomes, Doc Dreamy wins. Lower scores look great on paper. But that association doesn't mean what you think it means. Correlation is not causation, we know that, but we forget it every single time because our brains are wired to see patterns and assign causes.

The real statistic to pay attention to here is patient severity. It makes Doc Dreamy look good by sending him easier cases while sending Doc Duck the harder ones. When you don't randomize, you can't tell the difference between "Doc Dreamy is a better surgeon" and "Doc Dreamy got lucky with his patient assignment." 

If you can't randomize, and that's most of the time, you stratify. Look within groups of similar patient severity. Compare apples to apples, not apples to oranges. It's more work, and it's less satisfying than a clean randomized trial, but it's better than being fooled by confounding.

